{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAChlAEpg1HT"
      },
      "source": [
        "# Setup workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSb4r6aKWwAi",
        "outputId": "4efa1e79-885a-4358-a677-3005278c7a42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Volume in drive C is Windows\n",
            " Volume Serial Number is 647D-6841\n",
            "\n",
            " Directory of c:\\Users\\debnathk\\Desktop\\study\\articles\\code\\DeepDTA\\source\n",
            "\n",
            "01/30/2024  04:28 PM    <DIR>          .\n",
            "01/30/2024  03:04 PM    <DIR>          ..\n",
            "01/30/2024  04:50 PM    <DIR>          __pycache__\n",
            "01/30/2024  04:52 PM             2,831 arguments.py\n",
            "01/30/2024  03:04 PM            10,869 auc.jar\n",
            "01/30/2024  03:04 PM    <DIR>          data\n",
            "01/30/2024  03:04 PM             5,356 datahelper.py\n",
            "01/30/2024  03:04 PM             2,099 emetrics.py\n",
            "01/30/2024  03:04 PM    <DIR>          figures\n",
            "01/30/2024  03:04 PM               529 go.sh\n",
            "01/30/2024  03:04 PM    <DIR>          logs\n",
            "01/30/2024  04:52 PM            42,809 run_deepDTA.ipynb\n",
            "01/30/2024  03:04 PM            22,113 run_experiments.py\n",
            "               7 File(s)         86,606 bytes\n",
            "               6 Dir(s)  86,463,655,936 bytes free\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf8APvcQhFwS",
        "outputId": "ebe2790f-df23-4f0c-9a87-66318d9d4b30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\debnathk\\\\Desktop\\\\study\\\\articles\\\\code\\\\DeepDTA\\\\source'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUQFpyndd6Oq"
      },
      "source": [
        "# Workspace is all set - start working..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install --user tensorflow \n",
        "# %pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\debnathk\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\debnathk\\AppData\\Local\\Temp\\ipykernel_16820\\2149750015.py:16: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "#import matplotlib\n",
        "#matplotlib.use('Agg')\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "\n",
        "### We modified Pahikkala et al. (2014) source code for cross-val process ###\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "np.random.seed(1)\n",
        "rn.seed(1)\n",
        "\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "import keras\n",
        "from tensorflow.compat.v1.keras import backend as K\n",
        "tf.compat.v1.set_random_seed(0)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "\n",
        "from datahelper import *\n",
        "#import logging\n",
        "from itertools import product\n",
        "from arguments import argparser, logging\n",
        "\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Conv2D, GRU\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Masking, RepeatVector, Concatenate, Flatten\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.layers import Bidirectional\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras import optimizers, layers\n",
        "\n",
        "\n",
        "import sys, pickle, os\n",
        "import math, json, time\n",
        "import decimal\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from random import shuffle\n",
        "from copy import deepcopy\n",
        "from sklearn import preprocessing\n",
        "from emetrics import get_aupr, get_cindex, get_rm2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "TABSY = \"\\t\"\n",
        "figdir = \"figures/\"\n",
        "\n",
        "def build_combined_onehot(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2):\n",
        "    XDinput = Input(shape=(FLAGS.max_smi_len, FLAGS.charsmiset_size))\n",
        "    XTinput = Input(shape=(FLAGS.max_seq_len, FLAGS.charseqset_size))\n",
        "\n",
        "\n",
        "    encode_smiles= Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(XDinput)\n",
        "    encode_smiles = Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
        "    encode_smiles = Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
        "    encode_smiles = GlobalMaxPooling1D()(encode_smiles) #pool_size=pool_length[i]\n",
        "\n",
        "\n",
        "    encode_protein = Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(XTinput)\n",
        "    encode_protein = Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(encode_protein)\n",
        "    encode_protein = Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(encode_protein)\n",
        "    encode_protein = GlobalMaxPooling1D()(encode_protein)\n",
        "\n",
        "\n",
        "\n",
        "    encode_interaction = keras.layers.concatenate([encode_smiles, encode_protein])\n",
        "    #encode_interaction = keras.layers.concatenate([encode_smiles, encode_protein], axis=-1) #merge.Add()([encode_smiles, encode_protein])\n",
        "\n",
        "    # Fully connected \n",
        "    FC1 = Dense(1024, activation='relu')(encode_interaction)\n",
        "    FC2 = Dropout(0.1)(FC1)\n",
        "    FC2 = Dense(1024, activation='relu')(FC2)\n",
        "    FC2 = Dropout(0.1)(FC2)\n",
        "    FC2 = Dense(512, activation='relu')(FC2)\n",
        "\n",
        "\n",
        "    predictions = Dense(1, kernel_initializer='normal')(FC2) \n",
        "\n",
        "    interactionModel = Model(inputs=[XDinput, XTinput], outputs=[predictions])\n",
        "    interactionModel.compile(optimizer='adam', loss='mean_squared_error', metrics=[cindex_score]) #, metrics=['cindex_score']\n",
        "    \n",
        "\n",
        "    print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_combined_onehot.png')\n",
        "\n",
        "    return interactionModel\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_combined_categorical(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2):\n",
        "   \n",
        "    XDinput = Input(shape=(FLAGS.max_smi_len,), dtype='int32') ### Buralar flagdan gelmeliii\n",
        "    XTinput = Input(shape=(FLAGS.max_seq_len,), dtype='int32')\n",
        "\n",
        "    ### SMI_EMB_DINMS  FLAGS GELMELII \n",
        "    encode_smiles = Embedding(input_dim=FLAGS.charsmiset_size+1, output_dim=128, input_length=FLAGS.max_smi_len)(XDinput) \n",
        "    encode_smiles = Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
        "    encode_smiles = Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
        "    encode_smiles = Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
        "    encode_smiles = GlobalMaxPooling1D()(encode_smiles)\n",
        "\n",
        "\n",
        "    encode_protein = Embedding(input_dim=FLAGS.charseqset_size+1, output_dim=128, input_length=FLAGS.max_seq_len)(XTinput)\n",
        "    encode_protein = Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(encode_protein)\n",
        "    encode_protein = Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(encode_protein)\n",
        "    encode_protein = Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(encode_protein)\n",
        "    encode_protein = GlobalMaxPooling1D()(encode_protein)\n",
        "\n",
        "\n",
        "    encode_interaction = keras.layers.concatenate([encode_smiles, encode_protein], axis=-1) #merge.Add()([encode_smiles, encode_protein])\n",
        "\n",
        "    # Fully connected \n",
        "    FC1 = Dense(1024, activation='relu')(encode_interaction)\n",
        "    FC2 = Dropout(0.1)(FC1)\n",
        "    FC2 = Dense(1024, activation='relu')(FC2)\n",
        "    FC2 = Dropout(0.1)(FC2)\n",
        "    FC2 = Dense(512, activation='relu')(FC2)\n",
        "\n",
        "\n",
        "    # And add a logistic regression on top\n",
        "    predictions = Dense(1, kernel_initializer='normal')(FC2) #OR no activation, rght now it's between 0-1, do I want this??? activation='sigmoid'\n",
        "\n",
        "    interactionModel = Model(inputs=[XDinput, XTinput], outputs=[predictions])\n",
        "\n",
        "    interactionModel.compile(optimizer='adam', loss='mean_squared_error', metrics=[cindex_score]) #, metrics=['cindex_score']\n",
        "    print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_combined_categorical.png')\n",
        "\n",
        "    return interactionModel\n",
        "\n",
        "\n",
        "\n",
        "def build_single_drug(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2):\n",
        "   \n",
        "    interactionModel = Sequential()\n",
        "    XTmodel = Sequential()\n",
        "    XTmodel.add(Activation('linear', input_shape=(FLAGS.target_count,)))\n",
        "\n",
        "\n",
        "    encode_smiles = Sequential()\n",
        "    encode_smiles.add(Embedding(input_dim=FLAGS.charsmiset_size+1, output_dim=128, input_length=FLAGS.max_smi_len)) \n",
        "    encode_smiles.add(Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)) #input_shape=(MAX_SMI_LEN, SMI_EMBEDDING_DIMS)\n",
        "    encode_smiles.add(Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1))\n",
        "    encode_smiles.add(Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1))\n",
        "    encode_smiles.add(GlobalMaxPooling1D())\n",
        "\n",
        "\n",
        "    interactionModel.add(Merge([encode_smiles, XTmodel], mode='concat', concat_axis=1))\n",
        "    #interactionModel.add(layers.merge.Concatenate([XDmodel, XTmodel]))\n",
        "\n",
        "    # Fully connected \n",
        "    interactionModel.add(Dense(1024, activation='relu')) #1024\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(1024, activation='relu')) #1024\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(512, activation='relu')) \n",
        "\n",
        "\n",
        "    interactionModel.add(Dense(1, kernel_initializer='normal'))\n",
        "    interactionModel.compile(optimizer='adam', loss='mean_squared_error', metrics=[cindex_score])\n",
        "\n",
        "    print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_single_drug.png')\n",
        "\n",
        "    return interactionModel\n",
        "\n",
        "\n",
        "def build_single_prot(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2):\n",
        "   \n",
        "    interactionModel = Sequential()\n",
        "    XDmodel = Sequential()\n",
        "    XDmodel.add(Activation('linear', input_shape=(FLAGS.drugcount,)))\n",
        "\n",
        "\n",
        "    XTmodel1 = Sequential()\n",
        "    XTmodel1.add(Embedding(input_dim=FLAGS.charseqset_size+1, output_dim=128,  input_length=FLAGS.max_seq_len))\n",
        "    XTmodel1.add(Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)) #input_shape=(MAX_SEQ_LEN, SEQ_EMBEDDING_DIMS)\n",
        "    XTmodel1.add(Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1))\n",
        "    XTmodel1.add(Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1))\n",
        "    XTmodel1.add(GlobalMaxPooling1D())\n",
        "\n",
        "\n",
        "    interactionModel.add(Merge([XDmodel, XTmodel1], mode='concat', concat_axis=1))\n",
        "\n",
        "    # Fully connected \n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(512, activation='relu'))\n",
        "\n",
        "    interactionModel.add(Dense(1, kernel_initializer='normal'))\n",
        "    interactionModel.compile(optimizer='adam', loss='mean_squared_error', metrics=[cindex_score])\n",
        "\n",
        "    print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_single_protein.png')\n",
        "\n",
        "    return interactionModel\n",
        "\n",
        "def build_baseline(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2):\n",
        "    interactionModel = Sequential()\n",
        "\n",
        "    XDmodel = Sequential()\n",
        "    XDmodel.add(Dense(1, activation='linear', input_shape=(FLAGS.drug_count, )))\n",
        "\n",
        "    XTmodel = Sequential()\n",
        "    XTmodel.add(Dense(1, activation='linear', input_shape=(FLAGS.target_count,)))\n",
        "\n",
        "\n",
        "    interactionModel.add(Merge([XDmodel, XTmodel], mode='concat', concat_axis=1))\n",
        "\n",
        "    # Fully connected \n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(512, activation='relu'))\n",
        "\n",
        "    interactionModel.add(Dense(1, kernel_initializer='normal'))\n",
        "    interactionModel.compile(optimizer='adam', loss='mean_squared_error', metrics=[cindex_score])\n",
        "\n",
        "    print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_baseline.png')\n",
        "\n",
        "    return interactionModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nfold_1_2_3_setting_sample(XD, XT,  Y, label_row_inds, label_col_inds, measure, runmethod,  FLAGS, dataset):\n",
        "\n",
        "    bestparamlist = []\n",
        "    test_set, outer_train_sets = dataset.read_sets(FLAGS) \n",
        "    \n",
        "    foldinds = len(outer_train_sets)\n",
        "\n",
        "    test_sets = []\n",
        "    ## TRAIN AND VAL\n",
        "    val_sets = []\n",
        "    train_sets = []\n",
        "\n",
        "    #logger.info('Start training')\n",
        "    for val_foldind in range(foldinds):\n",
        "        val_fold = outer_train_sets[val_foldind]\n",
        "        val_sets.append(val_fold)\n",
        "        otherfolds = deepcopy(outer_train_sets)\n",
        "        otherfolds.pop(val_foldind)\n",
        "        otherfoldsinds = [item for sublist in otherfolds for item in sublist]\n",
        "        train_sets.append(otherfoldsinds)\n",
        "        test_sets.append(test_set)\n",
        "        print(\"val set\", str(len(val_fold)))\n",
        "        print(\"train set\", str(len(otherfoldsinds)))\n",
        "\n",
        "\n",
        "\n",
        "    bestparamind, best_param_list, bestperf, all_predictions_not_need, losses_not_need = general_nfold_cv(XD, XT,  Y, label_row_inds, label_col_inds, \n",
        "                                                                                                measure, runmethod, FLAGS, train_sets, val_sets)\n",
        "   \n",
        "    #print(\"Test Set len\", str(len(test_set)))\n",
        "    #print(\"Outer Train Set len\", str(len(outer_train_sets)))\n",
        "    bestparam, best_param_list, bestperf, all_predictions, all_losses = general_nfold_cv(XD, XT,  Y, label_row_inds, label_col_inds, \n",
        "                                                                                                measure, runmethod, FLAGS, train_sets, test_sets)\n",
        "    \n",
        "    testperf = all_predictions[bestparamind]##pointer pos \n",
        "\n",
        "    logging(\"---FINAL RESULTS-----\", FLAGS)\n",
        "    logging(\"best param index = %s,  best param = %.5f\" % \n",
        "            (bestparamind, bestparam), FLAGS)\n",
        "\n",
        "\n",
        "    testperfs = []\n",
        "    testloss= []\n",
        "\n",
        "    avgperf = 0.\n",
        "\n",
        "    for test_foldind in range(len(test_sets)):\n",
        "        foldperf = all_predictions[bestparamind][test_foldind]\n",
        "        foldloss = all_losses[bestparamind][test_foldind]\n",
        "        testperfs.append(foldperf)\n",
        "        testloss.append(foldloss)\n",
        "        avgperf += foldperf\n",
        "\n",
        "    avgperf = avgperf / len(test_sets)\n",
        "    avgloss = np.mean(testloss)\n",
        "    teststd = np.std(testperfs)\n",
        "\n",
        "    logging(\"Test Performance CI\", FLAGS)\n",
        "    logging(testperfs, FLAGS)\n",
        "    logging(\"Test Performance MSE\", FLAGS)\n",
        "    logging(testloss, FLAGS)\n",
        "\n",
        "    return avgperf, avgloss, teststd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def general_nfold_cv(XD, XT,  Y, label_row_inds, label_col_inds, prfmeasure, runmethod, FLAGS, labeled_sets, val_sets): ## BURAYA DA FLAGS LAZIM????\n",
        "    \n",
        "    paramset1 = FLAGS.num_windows                              #[32]#[32,  512] #[32, 128]  # filter numbers\n",
        "    paramset2 = FLAGS.smi_window_lengths                               #[4, 8]#[4,  32] #[4,  8] #filter length smi\n",
        "    paramset3 = FLAGS.seq_window_lengths                               #[8, 12]#[64,  256] #[64, 192]#[8, 192, 384]\n",
        "    epoch = FLAGS.num_epoch                                 #100\n",
        "    batchsz = FLAGS.batch_size                             #256\n",
        "\n",
        "    logging(\"---Parameter Search-----\", FLAGS)\n",
        "\n",
        "    w = len(val_sets)\n",
        "    h = len(paramset1) * len(paramset2) * len(paramset3)\n",
        "\n",
        "    all_predictions = [[0 for x in range(w)] for y in range(h)] \n",
        "    all_losses = [[0 for x in range(w)] for y in range(h)] \n",
        "    print(all_predictions)\n",
        "\n",
        "    for foldind in range(len(val_sets)):\n",
        "        valinds = val_sets[foldind]\n",
        "        labeledinds = labeled_sets[foldind]\n",
        "\n",
        "        Y_train = np.mat(np.copy(Y))\n",
        "\n",
        "        params = {}\n",
        "        XD_train = XD\n",
        "        XT_train = XT\n",
        "        trrows = label_row_inds[labeledinds]\n",
        "        trcols = label_col_inds[labeledinds]\n",
        "\n",
        "        XD_train = XD[trrows]\n",
        "        XT_train = XT[trcols]\n",
        "\n",
        "        train_drugs, train_prots,  train_Y = prepare_interaction_pairs(XD, XT, Y, trrows, trcols)\n",
        "        \n",
        "        terows = label_row_inds[valinds]\n",
        "        tecols = label_col_inds[valinds]\n",
        "        #print(\"terows\", str(terows), str(len(terows)))\n",
        "        #print(\"tecols\", str(tecols), str(len(tecols)))\n",
        "\n",
        "        val_drugs, val_prots,  val_Y = prepare_interaction_pairs(XD, XT,  Y, terows, tecols)\n",
        "\n",
        "\n",
        "        pointer = 0\n",
        "       \n",
        "        for param1ind in range(len(paramset1)): #hidden neurons\n",
        "            param1value = paramset1[param1ind]\n",
        "            for param2ind in range(len(paramset2)): #learning rate\n",
        "                param2value = paramset2[param2ind]\n",
        "\n",
        "                for param3ind in range(len(paramset3)):\n",
        "                    param3value = paramset3[param3ind]\n",
        "\n",
        "                    gridmodel = runmethod(FLAGS, param1value, param2value, param3value)\n",
        "                    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
        "                    gridres = gridmodel.fit(([np.array(train_drugs),np.array(train_prots) ]), np.array(train_Y), batch_size=batchsz, epochs=epoch, \n",
        "                            validation_data=( ([np.array(val_drugs), np.array(val_prots) ]), np.array(val_Y)),  shuffle=False, callbacks=[es] ) \n",
        "\n",
        "\n",
        "                    predicted_labels = gridmodel.predict([np.array(val_drugs), np.array(val_prots) ])\n",
        "                    loss, rperf2 = gridmodel.evaluate(([np.array(val_drugs),np.array(val_prots) ]), np.array(val_Y), verbose=0)\n",
        "                    rperf = prfmeasure(val_Y, predicted_labels)\n",
        "                    rperf = rperf[0]\n",
        "\n",
        "\n",
        "                    logging(\"P1 = %d,  P2 = %d, P3 = %d, Fold = %d, CI-i = %f, CI-ii = %f, MSE = %f\" % \n",
        "                    (param1ind, param2ind, param3ind, foldind, rperf, rperf2, loss), FLAGS)\n",
        "\n",
        "                    plotLoss(gridres, param1ind, param2ind, param3ind, foldind)\n",
        "\n",
        "                    all_predictions[pointer][foldind] =rperf #TODO FOR EACH VAL SET allpredictions[pointer][foldind]\n",
        "                    all_losses[pointer][foldind]= loss\n",
        "\n",
        "                    pointer +=1\n",
        "\n",
        "    bestperf = -float('Inf')\n",
        "    bestpointer = None\n",
        "\n",
        "\n",
        "    best_param_list = []\n",
        "    ##Take average according to folds, then chooose best params\n",
        "    pointer = 0\n",
        "    for param1ind in range(len(paramset1)):\n",
        "            for param2ind in range(len(paramset2)):\n",
        "                for param3ind in range(len(paramset3)):\n",
        "                \n",
        "                    avgperf = 0.\n",
        "                    for foldind in range(len(val_sets)):\n",
        "                        foldperf = all_predictions[pointer][foldind]\n",
        "                        avgperf += foldperf\n",
        "                    avgperf /= len(val_sets)\n",
        "                    #print(epoch, batchsz, avgperf)\n",
        "                    if avgperf > bestperf:\n",
        "                        bestperf = avgperf\n",
        "                        bestpointer = pointer\n",
        "                        best_param_list = [param1ind, param2ind, param3ind]\n",
        "\n",
        "                    pointer +=1\n",
        "        \n",
        "    return  bestpointer, best_param_list, bestperf, all_predictions, all_losses\n",
        "\n",
        "\n",
        "\n",
        "def cindex_score(y_true, y_pred):\n",
        "\n",
        "    g = tf.subtract(tf.expand_dims(y_pred, -1), y_pred)\n",
        "    g = tf.cast(g == 0.0, tf.float32) * 0.5 + tf.cast(g > 0.0, tf.float32)\n",
        "\n",
        "    f = tf.subtract(tf.expand_dims(y_true, -1), y_true) > 0.0\n",
        "    f = tf.compat.v1.matrix_band_part(tf.cast(f, tf.float32), -1, 0)\n",
        "\n",
        "    g = tf.reduce_sum(tf.multiply(g, f))\n",
        "    f = tf.reduce_sum(f)\n",
        "\n",
        "    return tf.where(tf.equal(g, 0), 0.0, g/f) #select\n",
        "\n",
        "\n",
        "   \n",
        "def plotLoss(history, batchind, epochind, param3ind, foldind):\n",
        "\n",
        "    figname = \"b\"+str(batchind) + \"_e\" + str(epochind) + \"_\" + str(param3ind) + \"_\"  + str( foldind) + \"_\" + str(time.time()) \n",
        "    plt.figure()\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "\t#plt.legend(['trainloss', 'valloss', 'cindex', 'valcindex'], loc='upper left')\n",
        "    plt.legend(['trainloss', 'valloss'], loc='upper left')\n",
        "    plt.savefig(\"figures/\"+figname +\".png\" , dpi=None, facecolor='w', edgecolor='w', orientation='portrait', \n",
        "                    format=None,transparent=False, bbox_inches=None, pad_inches=0.1,frameon=None)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    ## PLOT CINDEX\n",
        "    plt.figure()\n",
        "    plt.title('model concordance index')\n",
        "    plt.ylabel('cindex')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.plot(history.history['cindex_score'])\n",
        "    plt.plot(history.history['val_cindex_score'])\n",
        "    plt.legend(['traincindex', 'valcindex'], loc='upper left')\n",
        "    plt.savefig(\"figures/\"+figname + \"_acc.png\" , dpi=None, facecolor='w', edgecolor='w', orientation='portrait', \n",
        "                            papertype=None, format=None,transparent=False, bbox_inches=None, pad_inches=0.1,frameon=None)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "def prepare_interaction_pairs(XD, XT,  Y, rows, cols):\n",
        "    drugs = []\n",
        "    targets = []\n",
        "    targetscls = []\n",
        "    affinity=[] \n",
        "        \n",
        "    for pair_ind in range(len(rows)):\n",
        "        drug = XD[rows[pair_ind]]\n",
        "        drugs.append(drug)\n",
        "\n",
        "        target=XT[cols[pair_ind]]\n",
        "        targets.append(target)\n",
        "\n",
        "        affinity.append(Y[rows[pair_ind],cols[pair_ind]])\n",
        "\n",
        "    drug_data = np.stack(drugs)\n",
        "    target_data = np.stack(targets)\n",
        "\n",
        "    return drug_data,target_data,  affinity\n",
        "\n",
        "\n",
        "       \n",
        "def experiment(FLAGS, perfmeasure, deepmethod, foldcount=6): #5-fold cross validation + test\n",
        "\n",
        "    #Input\n",
        "    #XD: [drugs, features] sized array (features may also be similarities with other drugs\n",
        "    #XT: [targets, features] sized array (features may also be similarities with other targets\n",
        "    #Y: interaction values, can be real values or binary (+1, -1), insert value float(\"nan\") for unknown entries\n",
        "    #perfmeasure: function that takes as input a list of correct and predicted outputs, and returns performance\n",
        "    #higher values should be better, so if using error measures use instead e.g. the inverse -error(Y, P)\n",
        "    #foldcount: number of cross-validation folds for settings 1-3, setting 4 always runs 3x3 cross-validation\n",
        "\n",
        "\n",
        "    dataset = DataSet( fpath = FLAGS.dataset_path, ### BUNU ARGS DA GUNCELLE\n",
        "                      setting_no = FLAGS.problem_type, ##BUNU ARGS A EKLE\n",
        "                      seqlen = FLAGS.max_seq_len,\n",
        "                      smilen = FLAGS.max_smi_len,\n",
        "                      need_shuffle = False )\n",
        "    # set character set size\n",
        "    FLAGS.charseqset_size = dataset.charseqset_size \n",
        "    FLAGS.charsmiset_size = dataset.charsmiset_size \n",
        "\n",
        "    XD, XT, Y = dataset.parse_data(FLAGS)\n",
        "\n",
        "    XD = np.asarray(XD)\n",
        "    XT = np.asarray(XT)\n",
        "    Y = np.asarray(Y)\n",
        "\n",
        "    drugcount = XD.shape[0]\n",
        "    print(drugcount)\n",
        "    targetcount = XT.shape[0]\n",
        "    print(targetcount)\n",
        "\n",
        "    FLAGS.drug_count = drugcount\n",
        "    FLAGS.target_count = targetcount\n",
        "\n",
        "    label_row_inds, label_col_inds = np.where(np.isnan(Y)==False)  #basically finds the point address of affinity [x,y]\n",
        "\n",
        "    if not os.path.exists(figdir):\n",
        "        os.makedirs(figdir)\n",
        "\n",
        "    print(FLAGS.log_dir)\n",
        "    S1_avgperf, S1_avgloss, S1_teststd = nfold_1_2_3_setting_sample(XD, XT, Y, label_row_inds, label_col_inds,\n",
        "                                                                     perfmeasure, deepmethod, FLAGS, dataset)\n",
        "\n",
        "    logging(\"Setting \" + str(FLAGS.problem_type), FLAGS)\n",
        "    logging(\"avg_perf = %.5f,  avg_mse = %.5f, std = %.5f\" % \n",
        "            (S1_avgperf, S1_avgloss, S1_teststd), FLAGS)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_regression( FLAGS ): \n",
        "\n",
        "    perfmeasure = get_cindex\n",
        "    deepmethod = build_combined_categorical\n",
        "\n",
        "    experiment(FLAGS, perfmeasure, deepmethod)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read ./data/kiba/ start\n",
            "2111\n",
            "229\n",
            "/tmp1706651594.8315315/\n",
            "Reading ./data/kiba/ start\n",
            "val set 19709\n",
            "train set 78836\n",
            "val set 19709\n",
            "train set 78836\n",
            "val set 19709\n",
            "train set 78836\n",
            "val set 19709\n",
            "train set 78836\n",
            "val set 19709\n",
            "train set 78836\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(FLAGS\u001b[38;5;241m.\u001b[39mlog_dir)\n\u001b[0;32m      8\u001b[0m logging(\u001b[38;5;28mstr\u001b[39m(FLAGS), FLAGS)\n\u001b[1;32m----> 9\u001b[0m run_regression( FLAGS )\n",
            "Cell \u001b[1;32mIn[6], line 292\u001b[0m, in \u001b[0;36mrun_regression\u001b[1;34m(FLAGS)\u001b[0m\n\u001b[0;32m    289\u001b[0m perfmeasure \u001b[38;5;241m=\u001b[39m get_cindex\n\u001b[0;32m    290\u001b[0m deepmethod \u001b[38;5;241m=\u001b[39m build_combined_categorical\n\u001b[1;32m--> 292\u001b[0m experiment(FLAGS, perfmeasure, deepmethod)\n",
            "Cell \u001b[1;32mIn[6], line 277\u001b[0m, in \u001b[0;36mexperiment\u001b[1;34m(FLAGS, perfmeasure, deepmethod, foldcount)\u001b[0m\n\u001b[0;32m    274\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(figdir)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28mprint\u001b[39m(FLAGS\u001b[38;5;241m.\u001b[39mlog_dir)\n\u001b[1;32m--> 277\u001b[0m S1_avgperf, S1_avgloss, S1_teststd \u001b[38;5;241m=\u001b[39m nfold_1_2_3_setting_sample(XD, XT, Y, label_row_inds, label_col_inds,\n\u001b[0;32m    278\u001b[0m                                                                  perfmeasure, deepmethod, FLAGS, dataset)\n\u001b[0;32m    280\u001b[0m logging(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(FLAGS\u001b[38;5;241m.\u001b[39mproblem_type), FLAGS)\n\u001b[0;32m    281\u001b[0m logging(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_perf = \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m,  avg_mse = \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m, std = \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \n\u001b[0;32m    282\u001b[0m         (S1_avgperf, S1_avgloss, S1_teststd), FLAGS)\n",
            "Cell \u001b[1;32mIn[6], line 27\u001b[0m, in \u001b[0;36mnfold_1_2_3_setting_sample\u001b[1;34m(XD, XT, Y, label_row_inds, label_col_inds, measure, runmethod, FLAGS, dataset)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval set\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(val_fold)))\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain set\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(otherfoldsinds)))\n\u001b[1;32m---> 27\u001b[0m bestparamind, best_param_list, bestperf, all_predictions_not_need, losses_not_need \u001b[38;5;241m=\u001b[39m general_nfold_cv(XD, XT,  Y, label_row_inds, label_col_inds, \n\u001b[0;32m     28\u001b[0m                                                                                             measure, runmethod, FLAGS, train_sets, val_sets)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#print(\"Test Set len\", str(len(test_set)))\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#print(\"Outer Train Set len\", str(len(outer_train_sets)))\u001b[39;00m\n\u001b[0;32m     32\u001b[0m bestparam, best_param_list, bestperf, all_predictions, all_losses \u001b[38;5;241m=\u001b[39m general_nfold_cv(XD, XT,  Y, label_row_inds, label_col_inds, \n\u001b[0;32m     33\u001b[0m                                                                                             measure, runmethod, FLAGS, train_sets, test_sets)\n",
            "Cell \u001b[1;32mIn[6], line 79\u001b[0m, in \u001b[0;36mgeneral_nfold_cv\u001b[1;34m(XD, XT, Y, label_row_inds, label_col_inds, prfmeasure, runmethod, FLAGS, labeled_sets, val_sets)\u001b[0m\n\u001b[0;32m     76\u001b[0m logging(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---Parameter Search-----\u001b[39m\u001b[38;5;124m\"\u001b[39m, FLAGS)\n\u001b[0;32m     78\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_sets)\n\u001b[1;32m---> 79\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(paramset1) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(paramset2) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(paramset3)\n\u001b[0;32m     81\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(w)] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(h)] \n\u001b[0;32m     82\u001b[0m all_losses \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(w)] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(h)] \n",
            "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "if __name__==\"__main__\":\n",
        "    FLAGS = argparser()\n",
        "    FLAGS.log_dir = FLAGS.log_dir + str(time.time()) + \"/\"\n",
        "\n",
        "    if not os.path.exists(FLAGS.log_dir):\n",
        "        os.makedirs(FLAGS.log_dir)\n",
        "\n",
        "    logging(str(FLAGS), FLAGS)\n",
        "    run_regression( FLAGS )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4nSGIUHghU_"
      },
      "source": [
        "# Update github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZyHDLC-f3KW",
        "outputId": "8d8171ac-43f4-4783-ffce-6118540aedc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git restore <file>...\" to discard changes in working directory)\n",
            "\t\u001b[31mmodified:   run_experiments.py\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31m__pycache__/\u001b[m\n",
            "\t\u001b[31mfigures/\u001b[m\n",
            "\t\u001b[31mlogs/\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ],
      "source": [
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt8XPIOmf6aR"
      },
      "outputs": [],
      "source": [
        "!git add ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fz-JWCQmgE25",
        "outputId": "ad853482-32e4-4a07-9d50-85a988a20f9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[main 32acab2] Code modified\n",
            " 7 files changed, 13 insertions(+), 8 deletions(-)\n",
            " create mode 100644 code/DeepDTA/source/__pycache__/arguments.cpython-310.pyc\n",
            " create mode 100644 code/DeepDTA/source/__pycache__/datahelper.cpython-310.pyc\n",
            " create mode 100644 code/DeepDTA/source/__pycache__/emetrics.cpython-310.pyc\n",
            " create mode 100644 code/DeepDTA/source/figures/build_combined_categorical.png\n",
            " create mode 100644 code/DeepDTA/source/logs/1706120193.6732388/log.txt\n",
            " create mode 100644 code/DeepDTA/source/logs/1706120311.7540977/log.txt\n"
          ]
        }
      ],
      "source": [
        "!git commit -m \"Code modified\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uaFacMvgJUO",
        "outputId": "38e83552-7eaa-4f6f-9046-b2e95eb8ab94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enumerating objects: 22, done.\n",
            "Counting objects:   4% (1/22)\rCounting objects:   9% (2/22)\rCounting objects:  13% (3/22)\rCounting objects:  18% (4/22)\rCounting objects:  22% (5/22)\rCounting objects:  27% (6/22)\rCounting objects:  31% (7/22)\rCounting objects:  36% (8/22)\rCounting objects:  40% (9/22)\rCounting objects:  45% (10/22)\rCounting objects:  50% (11/22)\rCounting objects:  54% (12/22)\rCounting objects:  59% (13/22)\rCounting objects:  63% (14/22)\rCounting objects:  68% (15/22)\rCounting objects:  72% (16/22)\rCounting objects:  77% (17/22)\rCounting objects:  81% (18/22)\rCounting objects:  86% (19/22)\rCounting objects:  90% (20/22)\rCounting objects:  95% (21/22)\rCounting objects: 100% (22/22)\rCounting objects: 100% (22/22), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects:   6% (1/15)\rCompressing objects:  13% (2/15)\rCompressing objects:  20% (3/15)\rCompressing objects:  26% (4/15)\rCompressing objects:  33% (5/15)\rCompressing objects:  40% (6/15)\rCompressing objects:  46% (7/15)\rCompressing objects:  53% (8/15)\rCompressing objects:  60% (9/15)\rCompressing objects:  66% (10/15)\rCompressing objects:  73% (11/15)\rCompressing objects:  80% (12/15)\rCompressing objects:  86% (13/15)\rCompressing objects:  93% (14/15)\rCompressing objects: 100% (15/15)\rCompressing objects: 100% (15/15), done.\n",
            "Writing objects:   5% (1/17)\rWriting objects:  11% (2/17)\rWriting objects:  17% (3/17)\rWriting objects:  23% (4/17)\rWriting objects:  29% (5/17)\rWriting objects:  35% (6/17)\rWriting objects:  41% (7/17)\rWriting objects:  47% (8/17)\rWriting objects:  52% (9/17)\rWriting objects:  58% (10/17)\rWriting objects:  64% (11/17)\rWriting objects:  70% (12/17)\rWriting objects:  76% (13/17)\rWriting objects:  82% (14/17)\rWriting objects:  88% (15/17)\rWriting objects:  94% (16/17)\rWriting objects: 100% (17/17)\rWriting objects: 100% (17/17), 36.83 KiB | 7.37 MiB/s, done.\n",
            "Total 17 (delta 5), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (5/5), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/debnathk/articles\n",
            "   af34af0..32acab2  main -> main\n"
          ]
        }
      ],
      "source": [
        "!git push"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7Cw53kPgZV3"
      },
      "source": [
        "# Done!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
